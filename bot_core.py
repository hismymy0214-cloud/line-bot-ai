import os
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from collections import Counter

import pandas as pd

# =========================
# è¨­å®š
# =========================
DEFAULT_REPLY = "æŠ±æ­‰ï¼Œè©²è¨“ç·´æª”æ‰¾ä¸åˆ°ç¬¦åˆçš„è³‡æ–™ï¼Œè«‹é‡æ–°è¼¸å…¥æˆ–æ›å€‹èªªæ³•å–”ã€‚"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.environ.get("TRAINING_FILE", "training.xlsx")
DATA_PATH = os.path.join(BASE_DIR, DATA_FILE)

# è¦†è“‹ç‡é–€æª»ï¼škeywords è‡³å°‘ 80% è¢«ä½¿ç”¨è€…è¼¸å…¥ã€Œæ¶µè“‹ã€æ‰ç®—å‘½ä¸­
COVERAGE_THRESHOLD = float(os.environ.get("COVERAGE_THRESHOLD", "0.8"))

# æ¥è¿‘é–€æª»ï¼š>= 0.6 ä¸” < 0.8 æ™‚ï¼Œå›å€™é¸æç¤º
SUGGEST_THRESHOLD = float(os.environ.get("SUGGEST_THRESHOLD", "0.6"))
SUGGEST_TOPN = int(os.environ.get("SUGGEST_TOPN", "3"))

# å¤šå¹´åº¦æœ€å¤šå…è¨±æŸ¥è©¢å¤šå°‘å¹´ï¼ˆé¿å…æœ‰äººè¼¸å…¥ 80-120 å¹´æŠŠ Bot æ‰“çˆ†ï¼‰
MAX_YEAR_SPAN = int(os.environ.get("MAX_YEAR_SPAN", "10"))

# è¼¸å…¥å¤ªçŸ­æ™‚å…ˆå¼•å°
MIN_QUERY_LEN = int(os.environ.get("MIN_QUERY_LEN", "8"))

# å¹´åº¦å·®ç•°æ‘˜è¦ã€Œé–‹é—œã€é—œéµå­—ï¼šåªæœ‰å‡ºç¾é€™äº›å­—æ‰é¡¯ç¤ºæ‘˜è¦
ANALYSIS_KEYWORDS = ["æ¯”è¼ƒ", "è®ŠåŒ–", "ç•°å‹•", "å·®ç•°", "å¢æ¸›", "è¶¨å‹¢"]

# ===== æ»¿æ„åº¦èª¿æŸ¥ï¼ˆæŸ¥åˆ°/æŸ¥ä¸åˆ° åˆ†æµï¼‰=====
SURVEY_URL = os.environ.get("SURVEY_URL", "https://forms.gle/HCLmWz3br3egcBRN8")

SURVEY_FOOTER_SUCCESS = (
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
    "ğŸ“Š æ»¿æ„åº¦èª¿æŸ¥ï¼ˆç´„ 30 ç§’ï¼‰\n"
    "ç‚ºæŒçºŒç²¾é€²ã€ŒAI å·¥å‹™å±€ä¸»è¨ˆå•ç­”ç³»çµ±ã€ï¼Œèª æ‘¯é‚€è«‹æ‚¨å¡«å¯«ä½¿ç”¨é«”é©—å›é¥‹èˆ‡å»ºè­°ï¼š\n"
    f"ğŸ‘‰ {SURVEY_URL}\n"
    "ï¼ˆæœ¬å•å·ä¸è’é›†å€‹äººè³‡æ–™ï¼Œåƒ…ä½œç³»çµ±æ”¹å–„åƒè€ƒï¼Œæ„Ÿè¬æ‚¨çš„å”åŠ©ï¼‰"
)

SURVEY_FOOTER_FALLBACK = (
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
    "ğŸ“ å›é¥‹èˆ‡å»ºè­°\n"
    "è‹¥æœ¬ç³»çµ±å›è¦†ä¸ç¬¦åˆæ‚¨çš„æŸ¥è©¢éœ€æ±‚ï¼Œå¯å¦å”åŠ©æä¾›æ‚¨å¯¶è²´çš„å»ºè­°äº‹é …ï¼Œ"
    "ä½œç‚ºæœ¬ç³»çµ±å¾ŒçºŒç²¾é€²èˆ‡è³‡æ–™æ›´æ–°ä¹‹åƒè€ƒï¼š\n"
    f"ğŸ‘‰ {SURVEY_URL}"
)

# é¡å¤–ï¼šå¸¸è¦‹åŒç¾©/å¯«æ³•ä¿®æ­£ï¼ˆå¯å†æ“´å……ï¼‰
_REPLACEMENTS = [
    ("å¹´åº¦", "å¹´"),
    ("å¹´ åº¦", "å¹´"),
    ("ã€€", ""),  # å…¨å½¢ç©ºç™½
]

_PUNCT_RE = re.compile(r"[ï¼Œ,ã€‚ï¼ã€\s]+")
_YEAR_RE = re.compile(r"(?P<y>\d{3})\s*å¹´")


def _wants_summary(user_text: str) -> bool:
    """è¼¸å…¥å«ã€Œæ¯”è¼ƒ/è®ŠåŒ–/ç•°å‹•...ã€æ‰é¡¯ç¤ºå¹´åº¦å·®ç•°æ‘˜è¦ï¼ˆå«è¶¨å‹¢ä¸€å¥è©±ï¼‰ã€‚"""
    t = str(user_text or "")
    return any(k in t for k in ANALYSIS_KEYWORDS)


def _strip_analysis_keywords(text: str) -> str:
    """æŠŠã€æ¯”è¼ƒ/è®ŠåŒ–/ç•°å‹•...ã€ç­‰åˆ†æè©å¾æŸ¥è©¢ä¸­ç§»é™¤ï¼Œé¿å…å½±éŸ¿é¡Œåº«åŒ¹é…ã€‚"""
    t = str(text or "")
    for k in ANALYSIS_KEYWORDS:
        t = t.replace(k, "")
    return t.strip()


def _normalize(text: str) -> str:
    if text is None:
        return ""
    t = str(text).strip()
    for a, b in _REPLACEMENTS:
        t = t.replace(a, b)
    t = _PUNCT_RE.sub("", t)
    return t


def _extract_year(text: str) -> Optional[str]:
    """æŠ“ç¬¬ä¸€å€‹å¹´åº¦ï¼ˆ113å¹´ / 113ï¼‰"""
    m = _YEAR_RE.search(text)
    if m:
        return m.group("y")
    m2 = re.search(r"(?P<y>\d{3})", text)
    return m2.group("y") if m2 else None


def _strip_year(text_norm: str) -> str:
    t = _YEAR_RE.sub("", text_norm)
    t = re.sub(r"\d{3}", "", t)
    return t


def extract_years(text: str) -> List[int]:
    """
    æ”¯æ´å¤šå¹´åº¦è¼¸å…¥ï¼š
      - 112-113å¹´
      - 112~113å¹´
      - 112è‡³113å¹´ / 112åˆ°113å¹´
      - 112,113å¹´ / 112ã€113å¹´ï¼ˆæœƒå–å‡ºæ‰€æœ‰ä¸‰ä½æ•¸å¹´åº¦ï¼‰
    å›å‚³ï¼šå‡å†ªå¹´ä»½æ¸…å–®ï¼Œä¾‹å¦‚ [112, 113]
    """
    s = str(text or "")

    # 1) ç¯„åœï¼ˆå«ã€Œå¹´ã€æˆ–ä¸å«éƒ½å¯ï¼‰
    m = re.search(r"(\d{3})\s*[-~ï¼â€”]\s*(\d{3})\s*å¹´?", s)
    if not m:
        m = re.search(r"(\d{3})\s*(?:è‡³|åˆ°)\s*(\d{3})\s*å¹´?", s)

    if m:
        y1, y2 = int(m.group(1)), int(m.group(2))
        lo, hi = min(y1, y2), max(y1, y2)
        span = hi - lo + 1
        if span > MAX_YEAR_SPAN:
            # å¤ªé•·å°±ä¸åšå¤šå¹´åº¦ï¼ˆé¿å…è¢«æ¿«ç”¨ï¼‰ï¼Œå›ç©ºè®“å¾Œé¢èµ°å–®å¹´åº¦æç¤º
            return []
        return list(range(lo, hi + 1))

    # 2) éç¯„åœï¼šæŠ“å‡ºæ‰€æœ‰ä¸‰ä½æ•¸å¹´åº¦ï¼ˆå»é‡ï¼‰
    years = re.findall(r"(\d{3})\s*å¹´?", s)
    if years:
        uniq = sorted({int(y) for y in years})
        return uniq

    return []


def strip_year_expression(text: str) -> str:
    """
    æŠŠæ–‡å­—ä¸­çš„ã€Œå¹´åº¦è¡¨é”ã€ç§»é™¤ï¼Œç•™ä¸‹ã€Œä¸»é¡Œã€ï¼š
      112-113å¹´å·¥å‹™å±€æš¨æ‰€å±¬è·å“¡äººæ•¸ -> å·¥å‹™å±€æš¨æ‰€å±¬è·å“¡äººæ•¸
    """
    s = str(text or "")

    # å…ˆå»æ‰ç¯„åœ
    s = re.sub(r"\d{3}\s*[-~ï¼â€”]\s*\d{3}\s*å¹´?", "", s)
    s = re.sub(r"\d{3}\s*(?:è‡³|åˆ°)\s*\d{3}\s*å¹´?", "", s)

    # å†å»æ‰å–®ä¸€å¹´ï¼ˆé¿å…æ®˜ç•™ï¼‰
    s = re.sub(r"\d{3}\s*å¹´", "", s)
    s = re.sub(r"\d{3}", "", s)

    return s.strip()


@dataclass(frozen=True)
class Entry:
    keyword: str
    keyword_norm: str
    keyword_norm_noyear: str
    year: Optional[str]
    description: str
    source_url: str


_EXACT_MAP: Dict[str, Entry] = {}
_ENTRIES: List[Entry] = []


def _format_answer(entry: Entry) -> str:
    """
    LINE ä¸æ”¯æ´ markdown hyperlinkï¼Œä½†æœƒæŠŠç´”ç¶²å€è‡ªå‹•è½‰æˆå¯é»é€£çµï¼Œ
    æ‰€ä»¥ç”¨ã€Œå…§å®¹ + æ›è¡Œ + URLã€æœ€ç©©ã€‚
    """
    if entry.source_url:
        return f"{entry.description}\n{entry.source_url}"
    return entry.description


def _load_training() -> None:
    global _EXACT_MAP, _ENTRIES

    if not os.path.exists(DATA_PATH):
        print(f"[ERROR] training file not found: {DATA_PATH}")
        _EXACT_MAP = {}
        _ENTRIES = []
        return

    df = pd.read_excel(DATA_PATH, dtype=str).fillna("")
    cols = [c.strip().lower() for c in df.columns]
    colmap = {c.strip().lower(): c for c in df.columns}

    required = ["keywords", "description", "source_url"]
    missing = [c for c in required if c not in cols]
    if missing:
        print(f"[ERROR] training file missing columns: {missing}. Found: {list(df.columns)}")
        _EXACT_MAP = {}
        _ENTRIES = []
        return

    kw_col = colmap["keywords"]
    desc_col = colmap["description"]
    src_col = colmap["source_url"]

    exact_map: Dict[str, Entry] = {}
    entries: List[Entry] = []

    for _, r in df.iterrows():
        kw_raw = str(r.get(kw_col, "")).strip()
        desc = str(r.get(desc_col, "")).strip()
        src = str(r.get(src_col, "")).strip()

        if not kw_raw or not desc:
            continue

        kw_norm = _normalize(kw_raw)
        y = _extract_year(kw_raw)
        kw_norm_noyear = _strip_year(kw_norm)

        e = Entry(
            keyword=kw_raw,
            keyword_norm=kw_norm,
            keyword_norm_noyear=kw_norm_noyear,
            year=y,
            description=desc,
            source_url=src,
        )

        exact_map[kw_norm] = e
        entries.append(e)

    _EXACT_MAP = exact_map
    _ENTRIES = entries
    print(f"[DEBUG] training loaded: {DATA_PATH}, entries={len(_ENTRIES)}")


_load_training()


def _match_by_exact(user_text: str) -> Optional[Entry]:
    key = _normalize(user_text)
    if not key:
        return None
    return _EXACT_MAP.get(key)


def _coverage_ratio(keyword_norm: str, user_norm: str) -> float:
    if not keyword_norm:
        return 0.0
    kw = Counter(keyword_norm)
    us = Counter(user_norm)
    hit = sum(min(cnt, us.get(ch, 0)) for ch, cnt in kw.items())
    return hit / max(1, len(keyword_norm))


def _rank_matches(user_text: str, use_year_filter: bool = True) -> List[Tuple[float, int, Entry]]:
    user_norm = _normalize(user_text)
    if not user_norm:
        return []

    user_year = _extract_year(user_text) if use_year_filter else None

    candidates = _ENTRIES
    if user_year and use_year_filter:
        candidates = [e for e in candidates if e.year == user_year]

    ranked: List[Tuple[float, int, Entry]] = []
    for e in candidates:
        r = _coverage_ratio(e.keyword_norm, user_norm)
        tie = len(e.keyword_norm)
        ranked.append((r, tie, e))

    ranked.sort(key=lambda x: (x[0], x[1]), reverse=True)
    return ranked


def _rank_matches_noyear(user_text: str) -> List[Tuple[float, int, Entry]]:
    user_norm = _normalize(user_text)
    if not user_norm:
        return []

    user_norm_noyear = _strip_year(user_norm)
    if not user_norm_noyear:
        return []

    ranked: List[Tuple[float, int, Entry]] = []
    for e in _ENTRIES:
        r = _coverage_ratio(e.keyword_norm_noyear, user_norm_noyear)
        tie = len(e.keyword_norm_noyear)
        ranked.append((r, tie, e))

    ranked.sort(key=lambda x: (x[0], x[1]), reverse=True)
    return ranked


def build_reply_single_year(user_text: str) -> str:
    """
    å–®å¹´åº¦æŸ¥è©¢é‚è¼¯ï¼ˆå®Œæ•´ä¿ç•™ï¼‰ã€‚
    """
    text = (user_text or "").strip()
    if not text:
        return DEFAULT_REPLY

    user_norm = _normalize(text)
    user_year = _extract_year(text)

    # A) å¤ªçŸ­å…ˆå¼•å°
    if len(user_norm) < MIN_QUERY_LEN:
        return (
            "è«‹è¼¸å…¥æ›´å®Œæ•´çš„æŸ¥è©¢é—œéµè©ï¼ˆå«å¹´åº¦/å–®ä½/æŒ‡æ¨™ï¼‰ï¼Œä¾‹å¦‚ï¼š\n"
            "- 113å¹´å·¥å‹™å±€ä¸»ç®¡é ç®—æ•¸\n"
            "- 113å¹´å·¥å‹™å±€ä¸»ç®¡ç¶“å¸¸é–€\n"
            "- 113å¹´å·¥å‹™å±€æš¨æ‰€å±¬è·å“¡äººæ•¸"
        )

    # 1) å®Œå…¨ç¬¦åˆ
    e_exact = _match_by_exact(text)
    if e_exact:
        return _format_answer(e_exact)

    # 2) å¹´åº¦ä¸€è‡´ä¸‹çš„è¦†è“‹ç‡æ¯”å°
    ranked = _rank_matches(text, use_year_filter=True)
    if ranked:
        best_r, _, best_e = ranked[0]
        if best_r >= COVERAGE_THRESHOLD:
            return _format_answer(best_e)

    # C) å°‘æ‰“å¹´åº¦ï¼šåªæé†’è£œå¹´åº¦ï¼ˆä¸åˆ—å€™é¸ï¼‰
    if not user_year:
        ranked_noyear = _rank_matches_noyear(text)
        if ranked_noyear:
            best_r2, _, _ = ranked_noyear[0]
            if best_r2 >= COVERAGE_THRESHOLD:
                return (
                    "çœ‹èµ·ä¾†æ‚¨å¯èƒ½å°‘è¼¸å…¥ã€Œå¹´åº¦ã€ã€‚\n"
                    "è«‹åœ¨å•é¡Œå‰é¢åŠ ä¸Šå¹´åº¦ï¼ˆä¾‹å¦‚ï¼š113å¹´ï¼‰å†æŸ¥è©¢ä¸€æ¬¡ã€‚"
                )

    # B) é—œéµè©ä¸å¤ å®Œæ•´ï¼šåˆ—å‡ºæœ€æ¥è¿‘ 3 ç­†ï¼ˆä¸é¡¯ç¤ºç›¸ç¬¦ç‡ï¼‰
    if ranked:
        best_r, _, _ = ranked[0]
        if best_r >= SUGGEST_THRESHOLD:
            picks = ranked[:SUGGEST_TOPN]
            lines = "\n".join([f"- {e.keyword}" for _, _, e in picks])
            return (
                "æ‚¨æ˜¯ä¸æ˜¯è¦æ‰¾ä¸‹åˆ—è³‡æ–™ï¼š\n"
                f"{lines}\n"
                "ï¼ˆè‹¥éƒ½ä¸æ˜¯ï¼Œè«‹å†è£œå……æ›´å®Œæ•´çš„é—œéµè©ï¼Œä¾‹å¦‚ï¼šå¹´åº¦ï¼‹å–®ä½ï¼‹é …ç›®ï¼‹è·ç­‰/é–€åˆ¥ï¼‰"
            )

    return DEFAULT_REPLY


def _extract_total_people(ans_text: str) -> Optional[int]:
    """
    å¾å›è¦†æ–‡å­—ä¸­æŠ“ç¸½è¨ˆäººæ•¸ï¼ˆåªæŠ“ç¸½è¨ˆ/ç¸½æ•¸/åˆè¨ˆå¾Œé¢çš„æ•¸å­—ï¼‰ï¼š
    ä¾‹å¦‚ï¼šç¸½è¨ˆ524äºº / ç¸½æ•¸524äºº / åˆè¨ˆ524äºº
    """
    if not ans_text:
        return None
    m = re.search(r"(ç¸½è¨ˆ|ç¸½æ•¸|åˆè¨ˆ)\s*(\d+)\s*äºº", ans_text)
    if m:
        return int(m.group(2))
    return None


def _extract_first_url(ans_text: str) -> str:
    """å¾å›è¦†æ–‡å­—ä¸­æŠ“ç¬¬ä¸€å€‹ URLã€‚"""
    if not ans_text:
        return ""
    m = re.search(r"(https?://\S+)", ans_text)
    return m.group(1) if m else ""


def _extract_source_text_and_url(ans_text: str) -> Tuple[str, str]:
    """
    å¾å–®å¹´åº¦å›è¦†ä¸­æ“·å–ï¼š
    - è³‡æ–™ä¾†æºæ–‡å­—ï¼ˆä¾‹å¦‚ï¼šé«˜é›„å¸‚æ”¿åºœå·¥å‹™å±€æ€§åˆ¥çµ±è¨ˆå¹´å ±ã€‚ï¼‰
    - ç¬¬ä¸€å€‹ URL
    """
    if not ans_text:
        return "", ""

    lines = [l.strip() for l in ans_text.splitlines() if l.strip()]

    source_text = ""
    source_url = ""

    for i, line in enumerate(lines):
        # URL
        if not source_url:
            m = re.search(r"(https?://\S+)", line)
            if m:
                source_url = m.group(1)

        # ä¾†æºæ–‡å­—ï¼ˆé€šå¸¸åœ¨ã€Œ(è³‡æ–™ä¾†æº)ã€æˆ–ã€Œè³‡æ–™ä¾†æºã€å¾Œä¸€è¡Œï¼‰
        if "è³‡æ–™ä¾†æº" in line and i + 1 < len(lines):
            source_text = lines[i + 1]

    return source_text, source_url


def _format_multiyear_reply(
    years: List[int],
    year_to_text: Dict[int, Optional[str]],
    base_topic: str,
    show_summary: bool,
) -> str:
    """
    å¤šå¹´åº¦æ ¼å¼åŒ–ï¼ˆä¿æŒä½ ç›®å‰ç‰ˆæœ¬ï¼‰ï¼š
    - åªåˆ—å‡ºå„å¹´åº¦ã€Œç¸½è¨ˆã€(ç²¾ç°¡ç‰ˆ)
    - ç¼ºæ¼å¹´åº¦é›†ä¸­åˆ—ç¤º
    - å¿…è¦æ™‚é™„å¹´åº¦å·®ç•°æ‘˜è¦ï¼ˆä»ä»¥ã€Œç¸½è¨ˆã€è¨ˆç®—ï¼‰
    - è³‡æ–™ä¾†æºé¡¯ç¤ºä¸€æ¬¡ï¼ˆä¾†æºæ–‡å­— + URLï¼‰
    - é¸é …Aï¼šè‹¥ show_summary=Trueï¼Œé¡å¤–é™„ã€Œè¶¨å‹¢æ‘˜è¦ã€ä¸€å¥è©±
    """
    if not years:
        return DEFAULT_REPLY

    blocks: List[str] = []
    missing: List[int] = []

    totals: Dict[int, int] = {}

    source_text = ""
    source_url = ""

    def _format_multiyear_compact_line(year: int, base_topic2: str, total: Optional[int]) -> str:
        topic = (base_topic2 or "").strip()
        if topic.endswith("äººæ•¸"):
            topic = topic[:-2]
        if total is None:
            return f"ã€{year}å¹´ã€‘\n{year}å¹´{topic}ï¼ˆæŸ¥ç„¡ç¸½è¨ˆæ•¸å­—ï¼‰"
        return f"ã€{year}å¹´ã€‘\n{year}å¹´{topic}ç¸½è¨ˆ{total}äºº"

    def _trend_sentence_from_totals(years2: List[int], totals2: Dict[int, int]) -> str:
        ys = sorted([y for y in years2 if y in totals2])
        if len(ys) < 2:
            return ""

        first_y, last_y = ys[0], ys[-1]
        first_v, last_v = totals2[first_y], totals2[last_y]
        diff = last_v - first_v
        base = first_v if first_v != 0 else 1
        diff_pct = diff / base * 100.0

        series = [totals2[y] for y in ys]
        avg = sum(series) / max(1, len(series))
        rng = max(series) - min(series)
        vol_ratio = (rng / avg) if avg != 0 else 0.0

        if abs(diff_pct) < 1.0:
            overall = "æ•´é«”å¤§è‡´æŒå¹³"
        else:
            overall = "æ•´é«”å‘ˆç¾å°å¹…æˆé•·" if diff > 0 and abs(diff_pct) < 5.0 else (
                "æ•´é«”å‘ˆç¾æˆé•·" if diff > 0 else ("æ•´é«”å‘ˆç¾å°å¹…ä¸‹é™" if abs(diff_pct) < 5.0 else "æ•´é«”å‘ˆç¾ä¸‹é™")
            )

        if vol_ratio <= 0.03:
            volatility = "ç›¸å°ç©©å®š"
        elif vol_ratio <= 0.08:
            volatility = "å‘ˆç¾å°å¹…æ³¢å‹•"
        else:
            volatility = "æ³¢å‹•è¼ƒæ˜é¡¯"

        recent_phrase = ""
        if len(ys) >= 2:
            prev_y = ys[-2]
            prev_v = totals2[prev_y]
            recent_diff = last_v - prev_v
            if recent_diff > 0:
                recent_phrase = f"{last_y}å¹´è¼ƒå‰æœŸç•¥ç‚ºå›å‡"
            elif recent_diff < 0:
                recent_phrase = f"{last_y}å¹´è¼ƒå‰æœŸç•¥ç‚ºä¸‹æ»‘"
            else:
                recent_phrase = f"{last_y}å¹´èˆ‡å‰æœŸæŒå¹³"

        period = f"{first_y}â€“{last_y}å¹´"
        if overall == "æ•´é«”å¤§è‡´æŒå¹³":
            main = f"{period}æ•´é«”{volatility}"
        else:
            main = f"{period}{overall}ï¼Œèµ°å‹¢{volatility}" if volatility == "ç›¸å°ç©©å®š" else f"{period}æ•´é«”{volatility}"

        return f"ï¼ˆè¶¨å‹¢æ‘˜è¦ï¼‰\n{main}ï¼Œ{recent_phrase}ã€‚" if recent_phrase else f"ï¼ˆè¶¨å‹¢æ‘˜è¦ï¼‰\n{main}ã€‚"

    # å¹´åº¦è³‡æ–™ï¼ˆæ–°åˆ°èˆŠï¼‰
    for y in sorted(years, reverse=True):
        ans = year_to_text.get(y)
        if not ans or ans == DEFAULT_REPLY:
            missing.append(y)
            continue

        if not source_url and not source_text:
            st, su = _extract_source_text_and_url(ans)
            source_text = st
            source_url = su
        elif not source_url:
            source_url = _extract_first_url(ans)

        t = _extract_total_people(ans)
        if t is not None:
            totals[y] = t

        blocks.append(_format_multiyear_compact_line(y, base_topic, t))

    body = "\n\n".join(blocks) if blocks else "ï¼ˆæœ¬æ¬¡ç¯„åœå…§çš†æŸ¥ç„¡ç¬¦åˆè³‡æ–™ï¼‰"

    if missing:
        miss = "ã€".join([f"{m}å¹´" for m in sorted(missing, reverse=True)])
        body = f"{body}\n\nï¼ˆæŸ¥ç„¡è³‡æ–™å¹´åº¦ï¼š{miss}ï¼‰"

    if source_text or source_url:
        body = f"{body}\n\nï¼ˆè³‡æ–™ä¾†æºï¼‰"
        if source_text:
            body += f"\n{source_text}"
        if source_url:
            body += f"\n{source_url}"

    if show_summary and len(totals) >= 2:
        trend = _trend_sentence_from_totals(years, totals)
        if trend:
            body = f"{body}\n\n{trend}"

    if show_summary and len(totals) >= 2:
        ys = sorted(totals.keys())
        summary_lines = ["ï¼ˆå¹´åº¦å·®ç•°æ‘˜è¦ï¼‰"]
        for i in range(1, len(ys)):
            y1, y2 = ys[i - 1], ys[i]
            v1, v2 = totals[y1], totals[y2]
            diff = v2 - v1
            pct = (diff / v1 * 100) if v1 != 0 else 0.0
            sign = "+" if diff >= 0 else ""
            summary_lines.append(f"{y2}å¹´è¼ƒ{y1}å¹´ {sign}{diff}äººï¼ˆ{sign}{pct:.2f}%ï¼‰")
        body = f"{body}\n\n" + "\n".join(summary_lines)

    return body


# =========================
# footer åˆ†æµï¼šæŸ¥åˆ° / æŸ¥ä¸åˆ°
# =========================
def _is_success_reply(reply: str) -> bool:
    """
    åˆ¤æ–·ã€Œæ˜¯å¦æŸ¥åˆ°è³‡æ–™ã€ï¼š
    - DEFAULT_REPLY -> å¤±æ•—
    - å¼•å°/æé†’/å€™é¸ -> è¦–ç‚ºæœªæŸ¥åˆ°ï¼ˆä½¿ç”¨ fallback æ–‡æ¡ˆï¼‰
    - å¤šå¹´åº¦å…¨ç„¡ -> è¦–ç‚ºæœªæŸ¥åˆ°
    - å…¶é¤˜ -> è¦–ç‚ºæŸ¥åˆ°ï¼ˆä½¿ç”¨ success æ–‡æ¡ˆï¼‰
    """
    r = (reply or "").strip()
    if not r:
        return False

    if r == DEFAULT_REPLY:
        return False

    # å¼•å°/æé†’/å€™é¸ï¼ˆé€™äº›éƒ½ä¸ç®—ã€ŒæŸ¥åˆ°è³‡æ–™ã€ï¼‰
    if r.startswith("è«‹è¼¸å…¥æ›´å®Œæ•´çš„æŸ¥è©¢é—œéµè©"):
        return False
    if r.startswith("çœ‹èµ·ä¾†æ‚¨å¯èƒ½å°‘è¼¸å…¥ã€Œå¹´åº¦ã€"):
        return False
    if r.startswith("æ‚¨æ˜¯ä¸æ˜¯è¦æ‰¾ä¸‹åˆ—è³‡æ–™ï¼š"):
        return False

    # å¤šå¹´åº¦å…¨ç„¡
    if "ï¼ˆæœ¬æ¬¡ç¯„åœå…§çš†æŸ¥ç„¡ç¬¦åˆè³‡æ–™ï¼‰" in r:
        return False

    return True


def _append_survey_footer(reply: str) -> str:
    """
    ä¾ã€ŒæŸ¥åˆ°/æŸ¥ä¸åˆ°ã€é™„ä¸Šä¸åŒæ–‡æ¡ˆï¼ˆé¿å…é‡è¤‡é™„åŠ ï¼‰ã€‚
    """
    r = (reply or "").rstrip()
    if SURVEY_URL in r:
        return r  # å·²é™„éå°±ä¸å†é™„

    footer = SURVEY_FOOTER_SUCCESS if _is_success_reply(r) else SURVEY_FOOTER_FALLBACK
    return f"{r}\n\n{footer}" if r else footer


def build_reply(user_text: str) -> str:
    """
    å¤šå¹´åº¦å…¥å£ï¼šåµæ¸¬åˆ°ã€Œå¹´åº¦ç¯„åœã€å°±æ‹†æˆå¤šç­†å–®å¹´åº¦æŸ¥è©¢ï¼Œæœ€å¾Œåˆä½µå›è¦†ã€‚
    å¦å‰‡èµ°å–®å¹´åº¦æµç¨‹ã€‚

    æœ€å¾Œä¾ã€ŒæŸ¥åˆ°/æŸ¥ä¸åˆ°ã€é™„ä¸Šä¸åŒæ»¿æ„åº¦/å»ºè­°æ–‡æ¡ˆã€‚
    """
    text = (user_text or "").strip()
    if not text:
        return _append_survey_footer(DEFAULT_REPLY)

    years = extract_years(text)

    # å¤šå¹´åº¦ï¼šè‡³å°‘ 2 å¹´æ‰é€²å…¥åˆä½µæ¨¡å¼
    if len(years) >= 2:
        show_summary = _wants_summary(text)

        cleaned = _strip_analysis_keywords(text)
        base_topic = strip_year_expression(cleaned)

        year_to_text: Dict[int, Optional[str]] = {}
        for y in years:
            q = f"{y}å¹´{base_topic}"
            year_to_text[y] = build_reply_single_year(q)

        reply = _format_multiyear_reply(years, year_to_text, base_topic, show_summary)
        return _append_survey_footer(reply)

    # å–®å¹´åº¦
    reply = build_reply_single_year(text)
    return _append_survey_footer(reply)
