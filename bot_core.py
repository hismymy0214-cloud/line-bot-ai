import os
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from collections import Counter

import pandas as pd

# =========================
# è¨­å®š
# =========================
DEFAULT_REPLY = "æŠ±æ­‰ï¼Œè©²è¨“ç·´æª”æ‰¾ä¸åˆ°ç¬¦åˆçš„è³‡æ–™ï¼Œè«‹é‡æ–°è¼¸å…¥æˆ–æ›å€‹èªªæ³•å–”ã€‚"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.environ.get("TRAINING_FILE", "training.xlsx")
DATA_PATH = os.path.join(BASE_DIR, DATA_FILE)

# è¦†è“‹ç‡é–€æª»ï¼škeywords è‡³å°‘ 80% è¢«ä½¿ç”¨è€…è¼¸å…¥ã€Œæ¶µè“‹ã€æ‰ç®—å‘½ä¸­
COVERAGE_THRESHOLD = float(os.environ.get("COVERAGE_THRESHOLD", "0.8"))

# æ¥è¿‘é–€æª»ï¼š>= 0.6 ä¸” < 0.8 æ™‚ï¼Œå›å€™é¸æç¤º
SUGGEST_THRESHOLD = float(os.environ.get("SUGGEST_THRESHOLD", "0.6"))
SUGGEST_TOPN = int(os.environ.get("SUGGEST_TOPN", "3"))

# å¤šå¹´åº¦æœ€å¤šå…è¨±æŸ¥è©¢å¤šå°‘å¹´ï¼ˆé¿å…æœ‰äººè¼¸å…¥ 80-120 å¹´æŠŠ Bot æ‰“çˆ†ï¼‰
MAX_YEAR_SPAN = int(os.environ.get("MAX_YEAR_SPAN", "10"))

# è¼¸å…¥å¤ªçŸ­æ™‚å…ˆå¼•å°
MIN_QUERY_LEN = int(os.environ.get("MIN_QUERY_LEN", "8"))

# å¹´åº¦å·®ç•°æ‘˜è¦ã€Œé–‹é—œã€é—œéµå­—ï¼šåªæœ‰å‡ºç¾é€™äº›å­—æ‰é¡¯ç¤ºæ‘˜è¦
ANALYSIS_KEYWORDS = ["æ¯”è¼ƒ", "è®ŠåŒ–", "ç•°å‹•", "å·®ç•°", "å¢æ¸›", "è¶¨å‹¢"]

# ===== æŸ¥è©¢çµæœæ¨™é ­ =====
RESULT_HEADER = "æŸ¥è©¢çµæœå¦‚ä¸‹ï¼š"

# ===== æ»¿æ„åº¦èª¿æŸ¥ï¼ˆæŸ¥åˆ°/æŸ¥ä¸åˆ° åˆ†æµï¼‰=====
SURVEY_URL = os.environ.get("SURVEY_URL", "https://forms.gle/EzvDwUyq5A8sCQrS7")

SURVEY_FOOTER_SUCCESS = (
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
    "ğŸ“Š æ»¿æ„åº¦èª¿æŸ¥ï¼ˆç´„ 30 ç§’ï¼‰\n"
    "ç‚ºæŒçºŒç²¾é€²ã€ŒAI å·¥å‹™å±€ä¸»è¨ˆå•ç­”ç³»çµ±ã€ï¼Œèª æ‘¯é‚€è«‹æ‚¨å¡«å¯«ä½¿ç”¨é«”é©—å›é¥‹èˆ‡å»ºè­°ï¼š\n"
    f"ğŸ‘‰ {SURVEY_URL}\n"
    "ï¼ˆæœ¬å•å·ä¸è’é›†å€‹äººè³‡æ–™ï¼Œåƒ…ä½œç³»çµ±æ”¹å–„åƒè€ƒï¼Œæ„Ÿè¬æ‚¨çš„å”åŠ©ï¼‰"
)

SURVEY_FOOTER_FALLBACK = (
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
    "ğŸ“ å›é¥‹èˆ‡å»ºè­°\n"
    "è‹¥æœ¬ç³»çµ±å›è¦†ä¸ç¬¦åˆæ‚¨çš„æŸ¥è©¢éœ€æ±‚ï¼Œå¯å¦å”åŠ©æä¾›æ‚¨å¯¶è²´çš„å»ºè­°äº‹é …ï¼Œ"
    "ä½œç‚ºæœ¬ç³»çµ±å¾ŒçºŒç²¾é€²èˆ‡è³‡æ–™æ›´æ–°ä¹‹åƒè€ƒï¼š\n"
    f"ğŸ‘‰ {SURVEY_URL}"
)

# é¡å¤–ï¼šå¸¸è¦‹åŒç¾©/å¯«æ³•ä¿®æ­£ï¼ˆå¯å†æ“´å……ï¼‰
_REPLACEMENTS = [
    ("å¹´åº¦", "å¹´"),
    ("å¹´ åº¦", "å¹´"),
    ("ã€€", ""),  # å…¨å½¢ç©ºç™½
]

_PUNCT_RE = re.compile(r"[ï¼Œ,ã€‚ï¼ã€\s]+")
_YEAR_RE = re.compile(r"(?P<y>\d{3})\s*å¹´")


def _wants_summary(user_text: str) -> bool:
    """è¼¸å…¥å«ã€Œæ¯”è¼ƒ/è®ŠåŒ–/ç•°å‹•...ã€æ‰é¡¯ç¤ºå¹´åº¦å·®ç•°æ‘˜è¦ï¼ˆå«è¶¨å‹¢ä¸€å¥è©±ï¼‰ã€‚"""
    t = str(user_text or "")
    return any(k in t for k in ANALYSIS_KEYWORDS)


def _strip_analysis_keywords(text: str) -> str:
    """æŠŠã€æ¯”è¼ƒ/è®ŠåŒ–/ç•°å‹•...ã€ç­‰åˆ†æè©å¾æŸ¥è©¢ä¸­ç§»é™¤ï¼Œé¿å…å½±éŸ¿é¡Œåº«åŒ¹é…ã€‚"""
    t = str(text or "")
    for k in ANALYSIS_KEYWORDS:
        t = t.replace(k, "")
    return t.strip()


def _normalize(text: str) -> str:
    if text is None:
        return ""
    t = str(text).strip()
    for a, b in _REPLACEMENTS:
        t = t.replace(a, b)
    t = _PUNCT_RE.sub("", t)
    return t


def _extract_year(text: str) -> Optional[str]:
    """æŠ“ç¬¬ä¸€å€‹å¹´åº¦ï¼ˆ113å¹´ / 113ï¼‰"""
    m = _YEAR_RE.search(text)
    if m:
        return m.group("y")
    m2 = re.search(r"(?P<y>\d{3})", text)
    return m2.group("y") if m2 else None


def _strip_year(text_norm: str) -> str:
    t = _YEAR_RE.sub("", text_norm)
    t = re.sub(r"\d{3}", "", t)
    return t


def extract_years(text: str) -> List[int]:
    """
    æ”¯æ´å¤šå¹´åº¦è¼¸å…¥ï¼š
      - 112-113å¹´
      - 112~113å¹´
      - 112è‡³113å¹´ / 112åˆ°113å¹´
      - 112,113å¹´ / 112ã€113å¹´ï¼ˆæœƒå–å‡ºæ‰€æœ‰ä¸‰ä½æ•¸å¹´åº¦ï¼‰
    å›å‚³ï¼šå‡å†ªå¹´ä»½æ¸…å–®ï¼Œä¾‹å¦‚ [112, 113]
    """
    s = str(text or "")

    # 1) ç¯„åœï¼ˆå«ã€Œå¹´ã€æˆ–ä¸å«éƒ½å¯ï¼‰
    m = re.search(r"(\d{3})\s*[-~ï¼â€”]\s*(\d{3})\s*å¹´?", s)
    if not m:
        m = re.search(r"(\d{3})\s*(?:è‡³|åˆ°)\s*(\d{3})\s*å¹´?", s)

    if m:
        y1, y2 = int(m.group(1)), int(m.group(2))
        lo, hi = min(y1, y2), max(y1, y2)
        span = hi - lo + 1
        if span > MAX_YEAR_SPAN:
            return []
        return list(range(lo, hi + 1))

    # 2) éç¯„åœï¼šæŠ“å‡ºæ‰€æœ‰ä¸‰ä½æ•¸å¹´åº¦ï¼ˆå»é‡ï¼‰
    years = re.findall(r"(\d{3})\s*å¹´?", s)
    if years:
        uniq = sorted({int(y) for y in years})
        return uniq

    return []


def strip_year_expression(text: str) -> str:
    """
    æŠŠæ–‡å­—ä¸­çš„ã€Œå¹´åº¦è¡¨é”ã€ç§»é™¤ï¼Œç•™ä¸‹ã€Œä¸»é¡Œã€ï¼š
      112-113å¹´å·¥å‹™å±€æš¨æ‰€å±¬è·å“¡äººæ•¸ -> å·¥å‹™å±€æš¨æ‰€å±¬è·å“¡äººæ•¸
    """
    s = str(text or "")

    # å…ˆå»æ‰ç¯„åœ
    s = re.sub(r"\d{3}\s*[-~ï¼â€”]\s*\d{3}\s*å¹´?", "", s)
    s = re.sub(r"\d{3}\s*(?:è‡³|åˆ°)\s*\d{3}\s*å¹´?", "", s)

    # å†å»æ‰å–®ä¸€å¹´ï¼ˆé¿å…æ®˜ç•™ï¼‰
    s = re.sub(r"\d{3}\s*å¹´", "", s)
    s = re.sub(r"\d{3}", "", s)

    return s.strip()


@dataclass(frozen=True)
class Entry:
    keyword: str
    keyword_norm: str
    keyword_norm_noyear: str
    year: Optional[str]
    description: str
    unit: str
    source_url: str


_EXACT_MAP: Dict[str, Entry] = {}
_ENTRIES: List[Entry] = []


def _format_answer(entry: Entry) -> str:
    """
    LINE ä¸æ”¯æ´ markdown hyperlinkï¼Œä½†æœƒæŠŠç´”ç¶²å€è‡ªå‹•è½‰æˆå¯é»é€£çµï¼Œ
    æ‰€ä»¥ç”¨ã€Œå…§å®¹ + æ›è¡Œ + URLã€æœ€ç©©ã€‚
    """
    if entry.source_url:
        return f"{entry.description}\n{entry.source_url}"
    return entry.description


def _load_training() -> None:
    global _EXACT_MAP, _ENTRIES

    if not os.path.exists(DATA_PATH):
        print(f"[ERROR] training file not found: {DATA_PATH}")
        _EXACT_MAP = {}
        _ENTRIES = []
        return

    df = pd.read_excel(DATA_PATH, dtype=str).fillna("")
    cols = [c.strip().lower() for c in df.columns]
    colmap = {c.strip().lower(): c for c in df.columns}

    required = ["keywords", "description", "source_url"]
    missing = [c for c in required if c not in cols]
    if missing:
        print(f"[ERROR] training file missing columns: {missing}. Found: {list(df.columns)}")
        _EXACT_MAP = {}
        _ENTRIES = []
        return

    kw_col = colmap["keywords"]
    desc_col = colmap["description"]
    src_col = colmap["source_url"]
    unit_col = colmap.get("unit")  # unit æ¬„å¯æœ‰å¯ç„¡

    exact_map: Dict[str, Entry] = {}
    entries: List[Entry] = []

    for _, r in df.iterrows():
        kw_raw = str(r.get(kw_col, "")).strip()
        desc = str(r.get(desc_col, "")).strip()
        src = str(r.get(src_col, "")).strip()
        unit = str(r.get(unit_col, "")).strip() if unit_col else ""

        if not kw_raw or not desc:
            continue

        kw_norm = _normalize(kw_raw)
        y = _extract_year(kw_raw)
        kw_norm_noyear = _strip_year(kw_norm)

        e = Entry(
            keyword=kw_raw,
            keyword_norm=kw_norm,
            keyword_norm_noyear=kw_norm_noyear,
            year=y,
            description=desc,
            unit=unit,
            source_url=src,
        )

        exact_map[kw_norm] = e
        entries.append(e)

    _EXACT_MAP = exact_map
    _ENTRIES = entries
    print(f"[DEBUG] training loaded: {DATA_PATH}, entries={len(_ENTRIES)}")


_load_training()


def _match_by_exact(user_text: str) -> Optional[Entry]:
    key = _normalize(user_text)
    if not key:
        return None
    return _EXACT_MAP.get(key)


def _coverage_ratio(keyword_norm: str, user_norm: str) -> float:
    if not keyword_norm:
        return 0.0
    kw = Counter(keyword_norm)
    us = Counter(user_norm)
    hit = sum(min(cnt, us.get(ch, 0)) for ch, cnt in kw.items())
    return hit / max(1, len(keyword_norm))


def _rank_matches(user_text: str, use_year_filter: bool = True) -> List[Tuple[float, int, Entry]]:
    user_norm = _normalize(user_text)
    if not user_norm:
        return []

    user_year = _extract_year(user_text) if use_year_filter else None

    candidates = _ENTRIES
    if user_year and use_year_filter:
        candidates = [e for e in candidates if e.year == user_year]

    ranked: List[Tuple[float, int, Entry]] = []
    for e in candidates:
        r = _coverage_ratio(e.keyword_norm, user_norm)
        tie = len(e.keyword_norm)
        ranked.append((r, tie, e))

    ranked.sort(key=lambda x: (x[0], x[1]), reverse=True)
    return ranked


def _rank_matches_noyear(user_text: str) -> List[Tuple[float, int, Entry]]:
    user_norm = _normalize(user_text)
    if not user_norm:
        return []

    user_norm_noyear = _strip_year(user_norm)
    if not user_norm_noyear:
        return []

    ranked: List[Tuple[float, int, Entry]] = []
    for e in _ENTRIES:
        r = _coverage_ratio(e.keyword_norm_noyear, user_norm_noyear)
        tie = len(e.keyword_norm_noyear)
        ranked.append((r, tie, e))

    ranked.sort(key=lambda x: (x[0], x[1]), reverse=True)
    return ranked


def build_reply_single_year(user_text: str) -> str:
    """
    å–®å¹´åº¦æŸ¥è©¢é‚è¼¯ï¼ˆä¿ç•™åŸæœ¬äº’å‹•æ–‡æ¡ˆ/å€™é¸æç¤ºï¼‰ã€‚
    """
    text = (user_text or "").strip()
    if not text:
        return DEFAULT_REPLY

    user_norm = _normalize(text)
    user_year = _extract_year(text)

    # 1) å°‘æ‰“å¹´åº¦ï¼šå…ˆæé†’è£œå¹´åº¦ï¼ˆå„ªå…ˆæ–¼å¤ªçŸ­å¼•å°ï¼‰
    if not user_year:
        ranked_noyear = _rank_matches_noyear(text)
        if ranked_noyear:
            best_r2, _, _ = ranked_noyear[0]
            if best_r2 >= COVERAGE_THRESHOLD:
                return (
                    "çœ‹èµ·ä¾†æ‚¨å¯èƒ½å°‘è¼¸å…¥ã€Œå¹´åº¦ã€ã€‚\n"
                    "è«‹åœ¨å•é¡Œå‰é¢åŠ ä¸Šå¹´åº¦ï¼ˆä¾‹å¦‚ï¼š113å¹´ï¼‰å†æŸ¥è©¢ä¸€æ¬¡ã€‚"
                )

    # 2) å¤ªçŸ­å¼•å°
    if len(user_norm) < MIN_QUERY_LEN:
        return (
            "è«‹è¼¸å…¥æ›´å®Œæ•´çš„æŸ¥è©¢é—œéµè©ï¼ˆå«å¹´åº¦/å–®ä½/æŒ‡æ¨™ï¼‰ï¼Œä¾‹å¦‚ï¼š\n"
            "- 113å¹´å·¥å‹™å±€ä¸»ç®¡é ç®—æ•¸\n"
            "- 113å¹´å·¥å‹™å±€ä¸»ç®¡ç¶“å¸¸é–€\n"
            "- 113å¹´å·¥å‹™å±€æš¨æ‰€å±¬è·å“¡äººæ•¸"
        )

    # 3) å®Œå…¨ç¬¦åˆ
    e_exact = _match_by_exact(text)
    if e_exact:
        return _format_answer(e_exact)

    # 4) å¹´åº¦ä¸€è‡´ä¸‹çš„è¦†è“‹ç‡æ¯”å°
    ranked = _rank_matches(text, use_year_filter=True)
    if ranked:
        best_r, _, best_e = ranked[0]
        if best_r >= COVERAGE_THRESHOLD:
            return _format_answer(best_e)

    # 5) é—œéµè©ä¸å¤ å®Œæ•´ï¼šåˆ—å‡ºæœ€æ¥è¿‘ 3 ç­†ï¼ˆä¸é¡¯ç¤ºç›¸ç¬¦ç‡ï¼‰
    if ranked:
        best_r, _, _ = ranked[0]
        if best_r >= SUGGEST_THRESHOLD:
            picks = ranked[:SUGGEST_TOPN]
            lines = "\n".join([f"- {e.keyword}" for _, _, e in picks])
            return (
                "æ‚¨æ˜¯ä¸æ˜¯è¦æ‰¾ä¸‹åˆ—è³‡æ–™ï¼š\n"
                f"{lines}\n"
                "ï¼ˆè‹¥éƒ½ä¸æ˜¯ï¼Œè«‹å†è£œå……æ›´å®Œæ•´çš„é—œéµè©ï¼Œä¾‹å¦‚ï¼šå¹´åº¦ï¼‹å–®ä½ï¼‹é …ç›®ï¼‹è·ç­‰/é–€åˆ¥ï¼‰"
            )

    return DEFAULT_REPLY


# =========================
# å¤šå¹´åº¦å°ˆç”¨
# =========================
def _get_entry_for_year_query(query_text: str) -> Optional[Entry]:
    """
    å¤šå¹´åº¦ç”¨ï¼šçµ¦å®šã€Œå·²å«å¹´åº¦ã€çš„ queryï¼ˆä¾‹å¦‚ï¼š113å¹´å·¥å‹™å±€è·å“¡äººæ•¸ï¼‰ï¼Œ
    ç›´æ¥å›å‚³æœ€å¯èƒ½çš„ Entryï¼›æ‰¾ä¸åˆ°å°±å› Noneã€‚
    """
    text = (query_text or "").strip()
    if not text:
        return None

    # 1) å®Œå…¨ç¬¦åˆ
    e_exact = _match_by_exact(text)
    if e_exact:
        return e_exact

    # 2) å¹´åº¦ä¸€è‡´ä¸‹çš„è¦†è“‹ç‡æ¯”å°ï¼ˆå¤šå¹´åº¦ä¸åšå€™é¸æç¤ºï¼‰
    ranked = _rank_matches(text, use_year_filter=True)
    if ranked:
        best_r, _, best_e = ranked[0]
        if best_r >= COVERAGE_THRESHOLD:
            return best_e

    return None


def _extract_total_value(desc_text: str) -> Optional[int]:
    """å¾ description æŠ“ç¸½è¨ˆ/ç¸½æ•¸/åˆè¨ˆå¾Œé¢çš„æ•¸å­—ï¼ˆå…è¨±é€—è™Ÿï¼‰ã€‚"""
    if not desc_text:
        return None
    m = re.search(r"(ç¸½è¨ˆ|ç¸½æ•¸|åˆè¨ˆ)\s*([\d,]+)", desc_text)
    if not m:
        return None
    return int(m.group(2).replace(",", ""))


def _fallback_extract_unit(desc_text: str) -> str:
    """
    è‹¥ unit æ¬„æ²’å¡«ï¼Œå¾ description å˜—è©¦æŠ“çŸ­å–®ä½ï¼ˆé¿å…å®Œå…¨æ²’å–®ä½ï¼‰ã€‚
    ä¾‹ï¼šç¸½è¨ˆ524äºº / ç¸½è¨ˆ8,194,228åƒå…ƒ
    """
    if not desc_text:
        return ""
    m = re.search(r"(ç¸½è¨ˆ|ç¸½æ•¸|åˆè¨ˆ)\s*[\d,]+\s*([^\d\sï¼Œã€‚ï¼›;ã€()ï¼ˆï¼‰]{1,8})", desc_text)
    return (m.group(2) or "").strip() if m else ""


def _extract_first_url(ans_text: str) -> str:
    """å¾å›è¦†æ–‡å­—ä¸­æŠ“ç¬¬ä¸€å€‹ URLã€‚"""
    if not ans_text:
        return ""
    m = re.search(r"(https?://\S+)", ans_text)
    return m.group(1) if m else ""


def _extract_source_text_and_url(ans_text: str) -> Tuple[str, str]:
    """
    å¾å–®å¹´åº¦å›è¦†ä¸­æ“·å–ï¼š
    - è³‡æ–™ä¾†æºæ–‡å­—ï¼ˆä¾‹å¦‚ï¼šé«˜é›„å¸‚æ”¿åºœå·¥å‹™å±€æ€§åˆ¥çµ±è¨ˆå¹´å ±ã€‚ï¼‰
    - ç¬¬ä¸€å€‹ URL
    """
    if not ans_text:
        return "", ""

    lines = [l.strip() for l in ans_text.splitlines() if l.strip()]

    source_text = ""
    source_url = ""

    for i, line in enumerate(lines):
        if not source_url:
            m = re.search(r"(https?://\S+)", line)
            if m:
                source_url = m.group(1)

        if "è³‡æ–™ä¾†æº" in line and i + 1 < len(lines):
            source_text = lines[i + 1]

    return source_text, source_url


def _format_multiyear_reply(
    years: List[int],
    year_to_entry: Dict[int, Optional[Entry]],
    base_topic: str,
    show_summary: bool,
) -> str:
    """
    å¤šå¹´åº¦æ ¼å¼åŒ–ï¼š
    - ä¸€è¡Œä¸€å¹´åº¦ï¼š113å¹´XXXç¸½è¨ˆNN{unit}
    - ç¼ºæ¼å¹´åº¦é›†ä¸­åˆ—ç¤º
    - è³‡æ–™ä¾†æºé¡¯ç¤ºä¸€æ¬¡ï¼ˆä¾†æºæ–‡å­— + URLï¼‰
    - show_summary=Trueï¼šåŠ è¶¨å‹¢æ‘˜è¦ + å¹´åº¦å·®ç•°æ‘˜è¦ï¼ˆç”¨åŒä¸€å–®ä½ï¼‰
    """
    if not years:
        return DEFAULT_REPLY

    lines_out: List[str] = []
    missing: List[int] = []

    totals: Dict[int, int] = {}

    source_text = ""
    source_url = ""

    def _topic_no_suffix(topic: str) -> str:
        t = (topic or "").strip()
        # è‹¥ä¸»é¡Œä»¥ã€Œäººæ•¸ã€çµå°¾ï¼Œé¡¯ç¤ºæ™‚å»æ‰ã€Œäººæ•¸ã€
        if t.endswith("äººæ•¸"):
            t = t[:-2]
        return t

    def _format_multiyear_line(year: int, topic: str, total: Optional[int], unit: str) -> str:
        t = _topic_no_suffix(topic)
        if total is None:
            return f"{year}å¹´{t}ï¼ˆæŸ¥ç„¡ç¸½è¨ˆæ•¸å­—ï¼‰"
        # å¤šå¹´åº¦æ•¸å­—åŠ åƒåˆ†ä½
        return f"{year}å¹´{t}ç¸½è¨ˆ{total:,}{unit}"

    def _pick_summary_unit(years_sorted: List[int], unit_map: Dict[int, str]) -> str:
        for y in sorted(years_sorted, reverse=True):
            u = (unit_map.get(y) or "").strip()
            if u:
                return u
        return ""

    def _trend_sentence(years2: List[int], totals2: Dict[int, int], unit: str) -> str:
        ys = sorted([y for y in years2 if y in totals2])
        if len(ys) < 2:
            return ""

        first_y, last_y = ys[0], ys[-1]
        first_v, last_v = totals2[first_y], totals2[last_y]
        diff = last_v - first_v
        base = first_v if first_v != 0 else 1
        diff_pct = diff / base * 100.0

        series = [totals2[y] for y in ys]
        avg = sum(series) / max(1, len(series))
        rng = max(series) - min(series)
        vol_ratio = (rng / avg) if avg != 0 else 0.0

        if abs(diff_pct) < 1.0:
            overall = "æ•´é«”å¤§è‡´æŒå¹³"
        else:
            overall = (
                "æ•´é«”å‘ˆç¾å°å¹…æˆé•·"
                if diff > 0 and abs(diff_pct) < 5.0
                else (
                    "æ•´é«”å‘ˆç¾æˆé•·"
                    if diff > 0
                    else ("æ•´é«”å‘ˆç¾å°å¹…ä¸‹é™" if abs(diff_pct) < 5.0 else "æ•´é«”å‘ˆç¾ä¸‹é™")
                )
            )

        if vol_ratio <= 0.03:
            volatility = "ç›¸å°ç©©å®š"
        elif vol_ratio <= 0.08:
            volatility = "å‘ˆç¾å°å¹…æ³¢å‹•"
        else:
            volatility = "æ³¢å‹•è¼ƒæ˜é¡¯"

        recent_phrase = ""
        if len(ys) >= 2:
            prev_y = ys[-2]
            prev_v = totals2[prev_y]
            recent_diff = last_v - prev_v
            if recent_diff > 0:
                recent_phrase = f"{last_y}å¹´è¼ƒå‰æœŸç•¥ç‚ºå›å‡"
            elif recent_diff < 0:
                recent_phrase = f"{last_y}å¹´è¼ƒå‰æœŸç•¥ç‚ºä¸‹æ»‘"
            else:
                recent_phrase = f"{last_y}å¹´èˆ‡å‰æœŸæŒå¹³"

        period = f"{first_y}â€“{last_y}å¹´"
        if overall == "æ•´é«”å¤§è‡´æŒå¹³":
            main = f"{period}æ•´é«”{volatility}"
        else:
            main = f"{period}{overall}ï¼Œèµ°å‹¢{volatility}" if volatility == "ç›¸å°ç©©å®š" else f"{period}æ•´é«”{volatility}"

        unit_hint = f"ï¼ˆå–®ä½ï¼š{unit}ï¼‰" if unit else ""
        return (
            f"ï¼ˆè¶¨å‹¢æ‘˜è¦ï¼‰\n{main}ï¼Œ{recent_phrase}ã€‚{unit_hint}"
            if recent_phrase
            else f"ï¼ˆè¶¨å‹¢æ‘˜è¦ï¼‰\n{main}ã€‚{unit_hint}"
        )

    unit_map: Dict[int, str] = {}

    for y in sorted(years, reverse=True):
        e = year_to_entry.get(y)
        if not e:
            missing.append(y)
            continue

        if not source_url and not source_text:
            st, su = _extract_source_text_and_url(_format_answer(e))
            source_text = st
            source_url = su
        elif not source_url:
            source_url = _extract_first_url(_format_answer(e))

        total = _extract_total_value(e.description)
        unit = (e.unit or "").strip()
        if not unit:
            unit = _fallback_extract_unit(e.description)

        unit_map[y] = unit

        if total is not None:
            totals[y] = total

        lines_out.append(_format_multiyear_line(y, base_topic, total, unit))

    body = "\n".join(lines_out) if lines_out else "ï¼ˆæœ¬æ¬¡ç¯„åœå…§çš†æŸ¥ç„¡ç¬¦åˆè³‡æ–™ï¼‰"

    if missing:
        miss = "ã€".join([f"{m}å¹´" for m in sorted(missing, reverse=True)])
        body = f"{body}\n\nï¼ˆæŸ¥ç„¡è³‡æ–™å¹´åº¦ï¼š{miss}ï¼‰"

    if source_text or source_url:
        body = f"{body}\n\nï¼ˆè³‡æ–™ä¾†æºï¼‰"
        if source_text:
            body += f"\n{source_text}"
        if source_url:
            body += f"\n{source_url}"

    if show_summary and len(totals) >= 2:
        summary_unit = _pick_summary_unit(years, unit_map)
        trend = _trend_sentence(years, totals, summary_unit)
        if trend:
            body = f"{body}\n\n{trend}"

    if show_summary and len(totals) >= 2:
        ys = sorted(totals.keys())
        summary_unit = _pick_summary_unit(ys, unit_map)

        summary_lines = ["ï¼ˆå¹´åº¦å·®ç•°æ‘˜è¦ï¼‰"]
        for i in range(1, len(ys)):
            y1, y2 = ys[i - 1], ys[i]
            v1, v2 = totals[y1], totals[y2]
            diff = v2 - v1
            pct = (diff / v1 * 100) if v1 != 0 else 0.0
            sign = "+" if diff >= 0 else ""
            # å·®ç•°æ‘˜è¦ diff ä¹ŸåŠ åƒåˆ†ä½
            summary_lines.append(f"{y2}å¹´è¼ƒ{y1}å¹´ {sign}{diff:,}{summary_unit}ï¼ˆ{sign}{pct:.2f}%ï¼‰")

        body = f"{body}\n\n" + "\n".join(summary_lines)

    return body


# =========================
# footer åˆ†æµï¼šæŸ¥åˆ° / æŸ¥ä¸åˆ°
# =========================
def _is_success_reply(reply: str) -> bool:
    """
    åˆ¤æ–·ã€Œæ˜¯å¦æŸ¥åˆ°è³‡æ–™ã€ï¼š
    - DEFAULT_REPLY -> å¤±æ•—
    - å¼•å°/æé†’/å€™é¸ -> è¦–ç‚ºæœªæŸ¥åˆ°ï¼ˆä½¿ç”¨ fallback æ–‡æ¡ˆï¼‰
    - å¤šå¹´åº¦å…¨ç„¡ -> è¦–ç‚ºæœªæŸ¥åˆ°
    - å…¶é¤˜ -> è¦–ç‚ºæŸ¥åˆ°ï¼ˆä½¿ç”¨ success æ–‡æ¡ˆï¼‰
    """
    r = (reply or "").strip()
    if not r:
        return False

    if r == DEFAULT_REPLY:
        return False

    if r.startswith("è«‹è¼¸å…¥æ›´å®Œæ•´çš„æŸ¥è©¢é—œéµè©"):
        return False
    if r.startswith("çœ‹èµ·ä¾†æ‚¨å¯èƒ½å°‘è¼¸å…¥ã€Œå¹´åº¦ã€"):
        return False
    if r.startswith("æ‚¨æ˜¯ä¸æ˜¯è¦æ‰¾ä¸‹åˆ—è³‡æ–™ï¼š"):
        return False

    if "ï¼ˆæœ¬æ¬¡ç¯„åœå…§çš†æŸ¥ç„¡ç¬¦åˆè³‡æ–™ï¼‰" in r:
        return False

    return True


def _prepend_result_header(reply: str) -> str:
    """
    åªæœ‰ã€ŒæŸ¥åˆ°è³‡æ–™ã€æ™‚æ‰åŠ ä¸Šã€æŸ¥è©¢çµæœå¦‚ä¸‹ï¼šã€ï¼Œé¿å…å¹²æ“¾å¼•å°/å€™é¸è¨Šæ¯ã€‚
    ä¸”é¿å…é‡è¤‡åŠ æ¨™é ­ã€‚
    """
    r = (reply or "").strip()
    if not r:
        return r
    if r.startswith(RESULT_HEADER):
        return r
    if _is_success_reply(r):
        return f"{RESULT_HEADER}\n{r}"
    return r


def _append_survey_footer(reply: str) -> str:
    """
    ä¾ã€ŒæŸ¥åˆ°/æŸ¥ä¸åˆ°ã€é™„ä¸Šä¸åŒæ–‡æ¡ˆï¼ˆé¿å…é‡è¤‡é™„åŠ ï¼‰ã€‚
    """
    r = (reply or "").rstrip()
    if SURVEY_URL in r:
        return r  # å·²é™„éå°±ä¸å†é™„

    footer = SURVEY_FOOTER_SUCCESS if _is_success_reply(r) else SURVEY_FOOTER_FALLBACK
    return f"{r.rstrip()}\n{footer}" if r else footer


def build_reply(user_text: str) -> str:
    """
    å¤šå¹´åº¦å…¥å£ï¼šåµæ¸¬åˆ°ã€Œå¹´åº¦ç¯„åœã€å°±æ‹†æˆå¤šç­†å–®å¹´åº¦æŸ¥è©¢ï¼Œæœ€å¾Œåˆä½µå›è¦†ã€‚
    å¦å‰‡èµ°å–®å¹´åº¦æµç¨‹ã€‚

    æµç¨‹ï¼š
    1) ç”¢å‡º reply
    2) è‹¥æŸ¥åˆ°è³‡æ–™ â†’ å‰ç½®ã€ŒæŸ¥è©¢çµæœå¦‚ä¸‹ï¼šã€
    3) ä¾æŸ¥åˆ°/æŸ¥ä¸åˆ° â†’ é™„åŠ æ»¿æ„åº¦å•å· footer
    """
    text = (user_text or "").strip()
    if not text:
        return _append_survey_footer(DEFAULT_REPLY)

    years = extract_years(text)

    if len(years) >= 2:
        show_summary = _wants_summary(text)

        cleaned = _strip_analysis_keywords(text)
        base_topic = strip_year_expression(cleaned)

        year_to_entry: Dict[int, Optional[Entry]] = {}
        for y in years:
            q = f"{y}å¹´{base_topic}"
            year_to_entry[y] = _get_entry_for_year_query(q)

        reply = _format_multiyear_reply(years, year_to_entry, base_topic, show_summary)
        reply = _prepend_result_header(reply)
        return _append_survey_footer(reply)

    reply = build_reply_single_year(text)
    reply = _prepend_result_header(reply)
    return _append_survey_footer(reply)
